{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment-9.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CS354N\n",
        "---\n",
        "**Name: Somya Mehta**\n",
        "\n",
        "**Roll No.: 190001058**\n",
        "\n",
        "**Assignment - 9**\n",
        "\n"
      ],
      "metadata": {
        "id": "J_nq_brgB__n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hu6LVzh3eEY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "\n",
        "\n",
        "# ================================================================ #\n",
        "def shape(x):\n",
        "  return np.array(x).shape\n",
        "\n",
        "def collectData(f):\n",
        "  res = []\n",
        "  for line in f:\n",
        "    x = line.split(',')\n",
        "    y = []\n",
        "    for item in x:\n",
        "      y.append(float(item.strip()))\n",
        "    res.append(y)\n",
        "  return np.array(res)\n",
        "\n",
        "def initializeParameters(inputFeatures, neuronsInHiddenLayers, outputFeatures):\n",
        "  W1 = np.random.randn(neuronsInHiddenLayers, inputFeatures) * 0.01\n",
        "  W2 = np.random.randn(outputFeatures, neuronsInHiddenLayers) * 0.01\n",
        "  b1 = np.zeros((neuronsInHiddenLayers, 1))\n",
        "  b2 = np.zeros((outputFeatures, 1))\n",
        "\n",
        "  parameters = {\"W1\" : W1, \"b1\": b1,\n",
        "        \"W2\" : W2, \"b2\": b2}\n",
        "  return parameters\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_deriv(x):\n",
        "\t\t# compute the derivative of the sigmoid function ASSUMING\n",
        "\t\t# that x has already been passed through the 'sigmoid'\n",
        "\t\t# function\n",
        "\t\treturn x * (1 - x)\n",
        "\n",
        "def predict(X_test, params):\n",
        "  W1, W2, b1, b2 = params['W1'], params['W2'], params['b1'], params['b2']\n",
        "  A2 = []\n",
        "  for X in X_test:\n",
        "    X = X.reshape(-1, 1)\n",
        "    Z1 = np.dot(W1, X) + b1\n",
        "    A1 = sigmoid(Z1)\n",
        "    Z2 = np.dot(W2, A1) + b2\n",
        "    A2.append(sigmoid(Z2))\n",
        "    \n",
        "  return np.array(A2).reshape(-1, 1)\n",
        "\n",
        "def accuracy_score(y_pred, y_true):\n",
        "  c, n = 0.0, 0.0\n",
        "  for y, yt in zip(y_pred, y_true):\n",
        "    if y == yt: c += 1\n",
        "    n += 1\n",
        "  return c / n \n",
        "\n",
        "def forward_prop(X, params):\n",
        "    W1, W2, b1, b2 = params['W1'], params['W2'], params['b1'], params['b2']\n",
        "    Z1 = np.dot(W1, X) + b1\n",
        "    A1 = sigmoid(Z1)\n",
        "    Z2 = np.dot(W2, A1) + b2\n",
        "    A2 = sigmoid(Z2)\n",
        "    # here the cache is the data of previous iteration\n",
        "    # This will be used for backpropagation\n",
        "    cache = {\"Z1\": Z1,\n",
        "             \"A1\": A1,\n",
        "             \"Z2\": Z2,\n",
        "             \"A2\": A2}\n",
        "    return A2, cache\n",
        "  \n",
        "# Here Y is actual output\n",
        "def compute_cost(A2, Y):\n",
        "    # implementing the mean squared error\n",
        "    diff = Y - A2\n",
        "    diff = diff * diff\n",
        "    cost_sum = np.sum(diff)  \n",
        "    cost = 1/2 * cost_sum\n",
        "    cost = np.squeeze(cost)\n",
        "    return cost\n",
        "\n",
        "def back_propagate(X, Y, params, cache, learning_rate):\n",
        "  W1, W2, b1, b2 = params['W1'], params['W2'], params['b1'], params['b2']\n",
        "  # Retrieve also A1 and A2 from dictionary \"cache\"\n",
        "  A1 = cache['A1']\n",
        "  A2 = cache['A2']\n",
        "\n",
        "  # Backward propagation: calculate dW1, db1, dW2, db2.\n",
        "  # layer 1 from out\n",
        "  \n",
        "  dA2 = A2 - Y\n",
        "  dZ2 = dA2 * sigmoid_deriv(A2) # element wise multiplication\n",
        "  dW2 = np.dot(dZ2, A1.T)\n",
        "  db2 = np.sum(dZ2, axis = 1, keepdims = True)\n",
        "  # layer 2\n",
        "\n",
        "  dA1 = np.dot(W2.T, dZ2)\n",
        "  dZ1 = dA1 * sigmoid_deriv(A1)\n",
        "  dW1 = np.dot(dZ1, np.array(X).T)\n",
        "  db1 = np.sum(dZ1, axis = 1, keepdims = True)\n",
        "\n",
        "  # Updating the parameters according to algorithm\n",
        "  W1 = W1 - learning_rate * dW1\n",
        "  b1 = b1 - learning_rate * db1\n",
        "  W2 = W2 - learning_rate * dW2\n",
        "  b2 = b2 - learning_rate * db2\n",
        "\n",
        "  return W1, W2, b1, b2\n",
        "\n",
        "\n",
        "\n",
        "# Generate 7 input majority\n",
        "def generateData(n):\n",
        "\n",
        "  x = np.random.randint(0, 2, size=(n, 7))\n",
        "  y = []\n",
        "  for i in range(len(x)):\n",
        "    c = 0\n",
        "    for item in x[i]:\n",
        "      if item == 1: c += 1\n",
        "    if c >= 4:\n",
        "      y.append(1)\n",
        "    else:\n",
        "      y.append(0)\n",
        "  y = np.array(y)\n",
        "  return x, y\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "  \n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# encode labels\n",
        "X_train = np.zeros((20, 7))\n",
        "y_train = np.zeros((20, 1))\n",
        "X_test = np.zeros((10, 7))\n",
        "y_test = np.zeros((10, 1))\n",
        "\n",
        "X_train, y_train = generateData(len(X_train))\n",
        "X_test, y_test = generateData(len(X_test))\n",
        "# shape of data X_train (753, 64), X_test (363, 64), y_train (753, 1), y_test (363, 1)\n",
        "# training the model\n",
        "\n",
        "\n",
        "input_dims = len(X_train[0])\n",
        "output_dims = 1\n",
        "hidden_layer_dims = 15\n",
        "params = initializeParameters(input_dims, hidden_layer_dims, output_dims)\n",
        "# out, cache = forward_prop(X_train[0], params['W1'], params['W2'],params['b1'], params['b2'])\n",
        "# cost = compute_cost(out, y_train[0])\n",
        "# print(cost)\n",
        "\n",
        "num_iterations = 200\n",
        "learning_rate = 0.01\n",
        "\n",
        "for i in range(0, num_iterations):\n",
        "  for X, y in zip(X_train, y_train):\n",
        "    X = X.reshape(-1, 1)\n",
        "    y = y.reshape(-1, 1)\n",
        "    A2, cache = forward_prop(X, params)\n",
        "    cost = compute_cost(A2, y)\n",
        "    W1, W2, b1, b2 = back_propagate(X, y, params, cache, learning_rate)\n",
        "    params['W1'], params['W2'], params['b1'], params['b2'] = W1, W2, b1, b2\n",
        "  # Print the cost every 1000 iterations\n",
        "  if i % 25 == 0 or i == 199:\n",
        "      print (\"Cost after iteration % i: % f\" % (i, cost))\n",
        "\n",
        "# testing\n",
        "threshold = 0.5\n",
        "y_pred = predict(X_test, params)\n",
        "for i in range(0, y_pred.shape[0]):\n",
        "  if y_pred[i] >= threshold: y_pred[i] = 1\n",
        "  else: y_pred[i] = 0\n",
        "acc = accuracy_score(y_pred, y_test)  \n",
        "print(params['W1'])\n",
        "print(params['W2'])\n",
        "\n",
        "print(f\"accuracy is {acc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6igTzCGz5syP",
        "outputId": "3888a3f4-118a-4488-bcc0-f64df4cac6d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost after iteration  0:  0.129505\n",
            "Cost after iteration  25:  0.168801\n",
            "Cost after iteration  50:  0.178597\n",
            "Cost after iteration  75:  0.181103\n",
            "Cost after iteration  100:  0.181796\n",
            "Cost after iteration  125:  0.182033\n",
            "Cost after iteration  150:  0.182152\n",
            "Cost after iteration  175:  0.182242\n",
            "Cost after iteration  199:  0.182320\n",
            "[[-1.08341564e-03 -1.96908860e-03 -2.56837502e-02 -1.96968487e-02\n",
            "  -1.63404116e-02 -2.15603504e-02 -8.42483303e-06]\n",
            " [-1.35627806e-03  3.39537979e-03 -2.10430229e-02 -1.94188293e-02\n",
            "  -7.60192066e-03 -6.56017560e-04 -3.28145905e-03]\n",
            " [-5.77621234e-03 -1.54106850e-03 -2.19959763e-02  1.32391341e-02\n",
            "  -1.09876724e-02 -1.28070038e-02  3.96899672e-05]\n",
            " [-8.80705796e-03  7.26460782e-03 -7.24272897e-03 -2.20961597e-02\n",
            "  -2.96337264e-04 -2.08730889e-02 -4.70493248e-03]\n",
            " [ 3.54239195e-03 -4.67662671e-03 -1.60199287e-02  5.57666736e-03\n",
            "   6.70351153e-03  1.67216405e-04 -1.86756271e-02]\n",
            " [-1.35981751e-02 -1.52742139e-03 -1.12320915e-02  3.25519973e-03\n",
            "   1.01404719e-03 -5.47967347e-03 -7.93969573e-03]\n",
            " [-2.66938658e-04 -1.28303583e-02 -2.06342426e-02 -1.56789903e-02\n",
            "   4.26648754e-04 -3.52706479e-03  4.59015512e-03]\n",
            " [ 3.98864644e-03  2.51963503e-03 -5.76339643e-03 -1.30029423e-03\n",
            "   9.83272715e-04 -7.80812684e-03 -1.44925426e-02]\n",
            " [-1.29574482e-05  2.06705097e-02 -1.80394891e-02 -3.22565521e-02\n",
            "  -1.90973061e-02 -7.34162453e-03  1.22590232e-02]\n",
            " [ 8.59537233e-03 -5.39515219e-03 -7.02301199e-03 -3.74343171e-03\n",
            "   3.37467337e-03  6.42883879e-03 -3.38699244e-02]\n",
            " [ 6.02823088e-03 -1.09109020e-02  9.07229065e-03 -6.59746901e-03\n",
            "  -1.59600650e-02 -1.30840218e-02  1.63055388e-02]\n",
            " [ 1.09061653e-02  1.15843412e-02 -2.04576807e-02 -6.23760931e-03\n",
            "  -1.60421817e-04 -3.06861091e-04 -1.98893324e-03]\n",
            " [ 6.96359954e-03 -1.32752949e-02 -6.11147954e-03 -7.33086677e-03\n",
            "  -9.92724025e-03 -4.58700768e-03 -1.31630206e-02]\n",
            " [ 5.87663837e-03 -1.58252839e-02 -1.64351468e-02  1.14210024e-03\n",
            "  -9.30513453e-03 -8.95697669e-03 -1.71082868e-02]\n",
            " [ 6.92055160e-04 -1.81281242e-02 -1.90443783e-02 -1.03893928e-02\n",
            "  -6.39908020e-03 -6.81611574e-03  1.48085087e-02]]\n",
            "[[-0.0666497  -0.04309319 -0.04936951 -0.04004047 -0.04265996 -0.04326357\n",
            "  -0.04451069 -0.04697578 -0.05241833 -0.03951219 -0.03912299 -0.04553564\n",
            "  -0.03082072 -0.04841263 -0.04493484]]\n",
            "accuracy is 0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q2\n",
        "\n",
        "# ================================================================ #\n",
        "def shape(x):\n",
        "  return np.array(x).shape\n",
        "\n",
        "def collectData(f):\n",
        "  res = []\n",
        "  for line in f:\n",
        "    x = line.split(',')\n",
        "    y = []\n",
        "    for item in x:\n",
        "      y.append(float(item.strip()))\n",
        "    res.append(y)\n",
        "  return np.array(res)\n",
        "\n",
        "def initializeParameters(inputFeatures, neuronsInHiddenLayers, outputFeatures):\n",
        "  W1 = np.random.randn(neuronsInHiddenLayers, inputFeatures) * 0.01\n",
        "  W2 = np.random.randn(outputFeatures, neuronsInHiddenLayers) * 0.01\n",
        "  b1 = np.zeros((neuronsInHiddenLayers, 1))\n",
        "  b2 = np.zeros((outputFeatures, 1))\n",
        "\n",
        "  parameters = {\"W1\" : W1, \"b1\": b1,\n",
        "        \"W2\" : W2, \"b2\": b2}\n",
        "  return parameters\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_deriv(x):\n",
        "\t\t# compute the derivative of the sigmoid function ASSUMING\n",
        "\t\t# that x has already been passed through the 'sigmoid'\n",
        "\t\t# function\n",
        "\t\treturn x * (1 - x)\n",
        "\n",
        "def predict(X_test, params):\n",
        "  W1, W2, b1, b2 = params['W1'], params['W2'], params['b1'], params['b2']\n",
        "  A2 = []\n",
        "  for X in X_test:\n",
        "    X = X.reshape(-1, 1)\n",
        "    Z1 = np.dot(W1, X) + b1\n",
        "    A1 = sigmoid(Z1)\n",
        "    Z2 = np.dot(W2, A1) + b2\n",
        "    A2.append(sigmoid(Z2))\n",
        "    \n",
        "  return np.array(A2).reshape(-1, 1)\n",
        "\n",
        "def accuracy(predictions, targets):\n",
        "  return 0.5 * np.sum((predictions - targets) ** 2)\n",
        "\n",
        "def forward_prop(X, params):\n",
        "    W1, W2, b1, b2 = params['W1'], params['W2'], params['b1'], params['b2']\n",
        "    Z1 = np.dot(W1, X) + b1\n",
        "    A1 = sigmoid(Z1)\n",
        "    Z2 = np.dot(W2, A1) + b2\n",
        "    A2 = sigmoid(Z2)\n",
        "    # here the cache is the data of previous iteration\n",
        "    # This will be used for backpropagation\n",
        "    cache = {\"Z1\": Z1,\n",
        "             \"A1\": A1,\n",
        "             \"Z2\": Z2,\n",
        "             \"A2\": A2}\n",
        "    return A2, cache\n",
        "  \n",
        "# Here Y is actual output\n",
        "def compute_cost(A2, Y):\n",
        "    # implementing the mean squared error\n",
        "    diff = Y - A2\n",
        "    diff = diff * diff\n",
        "    cost_sum = np.sum(diff)  \n",
        "    cost = 1/2 * cost_sum\n",
        "    cost = np.squeeze(cost)\n",
        "    return cost\n",
        "\n",
        "def back_propagate(X, Y, params, cache, learning_rate):\n",
        "  W1, W2, b1, b2 = params['W1'], params['W2'], params['b1'], params['b2']\n",
        "  # Retrieve also A1 and A2 from dictionary \"cache\"\n",
        "  A1 = cache['A1']\n",
        "  A2 = cache['A2']\n",
        "\n",
        "  # Backward propagation: calculate dW1, db1, dW2, db2.\n",
        "  # layer 1 from out\n",
        "  \n",
        "  dA2 = A2 - Y\n",
        "  dZ2 = dA2 * sigmoid_deriv(A2) # element wise multiplication\n",
        "  dW2 = np.dot(dZ2, A1.T)\n",
        "  db2 = np.sum(dZ2, axis = 1, keepdims = True)\n",
        "  # layer 2\n",
        "\n",
        "  dA1 = np.dot(W2.T, dZ2)\n",
        "  dZ1 = dA1 * sigmoid_deriv(A1)\n",
        "  dW1 = np.dot(dZ1, np.array(X).T)\n",
        "  db1 = np.sum(dZ1, axis = 1, keepdims = True)\n",
        "\n",
        "  # Updating the parameters according to algorithm\n",
        "  W1 = W1 - learning_rate * dW1\n",
        "  b1 = b1 - learning_rate * db1\n",
        "  W2 = W2 - learning_rate * dW2\n",
        "  b2 = b2 - learning_rate * db2\n",
        "\n",
        "  return W1, W2, b1, b2\n",
        "\n",
        "def unison_shuffled_copies(a, b):\n",
        "    assert len(a) == len(b)\n",
        "    p = np.random.permutation(len(a))\n",
        "    return a[p], b[p]\n",
        "\n",
        "# Generate 7 input majority\n",
        "def input_1(n = 100):\n",
        "  Train = np.zeros(n)\n",
        "  Label = np.zeros(n)\n",
        "  for i in range(n):\n",
        "    Train[i] = i+1\n",
        "    Label[i] = 1 / Train[i]\n",
        "  X, y = unison_shuffled_copies(Train, Label)\n",
        "  len_train = int(0.7*n)\n",
        "  # len_test = int(0.3*n)\n",
        "  return X[0:len_train], y[0:len_train], X[len_train:], y[len_train:]\n",
        "\n",
        "  \n",
        "  \n",
        "# encode labels\n",
        "X_train, y_train, X_test, y_test = input_1()\n",
        "X_train = np.array(X_train)\n",
        "X_test = np.array(X_test)\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "X_train = X_train.reshape(-1,1)\n",
        "X_test = X_test.reshape(-1,1)\n",
        "y_train = y_train.reshape(-1,1)\n",
        "y_test = y_test.reshape(-1,1)\n",
        "\n",
        "input_dims = len(X_train[0])\n",
        "output_dims = 1\n",
        "print (\"enter number of neurons in hidden layer: \")\n",
        "hidden_layer_dims = int(input())\n",
        "params = initializeParameters(input_dims, hidden_layer_dims, output_dims)\n",
        "\n",
        "num_iterations = 1000\n",
        "learning_rate = 0.01\n",
        "\n",
        "for i in range(0, num_iterations):\n",
        "  for X, y in zip(X_train, y_train):\n",
        "    X = X.reshape(-1, 1)\n",
        "    y = y.reshape(-1, 1)\n",
        "    A2, cache = forward_prop(X, params)\n",
        "    cost = compute_cost(A2, y)\n",
        "    W1, W2, b1, b2 = back_propagate(X, y, params, cache, learning_rate)\n",
        "    params['W1'], params['W2'], params['b1'], params['b2'] = W1, W2, b1, b2\n",
        "  # Print the cost every 1000 iterations\n",
        "  if i % 100 == 0:\n",
        "      print (\"Cost after iteration % i: % f\" % (i, cost))\n",
        "\n",
        "# testing\n",
        "y_pred = predict(X_test, params)\n",
        "loss = accuracy(y_pred, y_test)  \n",
        "\n",
        "print(f\"Loss is {loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvBTonfkAkos",
        "outputId": "832c3e98-2905-49ff-8416-70d45f45d7cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter number of neurons in hidden layer: \n",
            "5\n",
            "Cost after iteration  0:  0.088319\n",
            "Cost after iteration  100:  0.000617\n",
            "Cost after iteration  200:  0.000483\n",
            "Cost after iteration  300:  0.000468\n",
            "Cost after iteration  400:  0.000461\n",
            "Cost after iteration  500:  0.000456\n",
            "Cost after iteration  600:  0.000449\n",
            "Cost after iteration  700:  0.000440\n",
            "Cost after iteration  800:  0.000430\n",
            "Cost after iteration  900:  0.000416\n",
            "Loss is 0.01640504788073724\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q3\n",
        "\n",
        "# ================================================================ #\n",
        "class NeuralNetwork:\n",
        "  def __init__(self, layers, alpha=0.1):\n",
        "    self.W = []\n",
        "    self.layers = layers\n",
        "    self.alpha = alpha\n",
        "    for i in np.arange(0, len(layers) - 2):\n",
        "      w = np.random.randn(layers[i] + 1, layers[i + 1] + 1)\n",
        "      self.W.append(w / np.sqrt(layers[i]))\n",
        "    w = np.random.randn(layers[-2] + 1, layers[-1])\n",
        "    self.W.append(w / np.sqrt(layers[-2]))\n",
        "  \n",
        "  def __repr__(self):\n",
        "    return \"NeuralNetwork: {}\".format(\n",
        "      \"-\".join(str(l) for l in self.layers))\n",
        "  \n",
        "  def sigmoid(self, x):\n",
        "    return 1.0 / (1 + np.exp(-x))\n",
        "\n",
        "  def sigmoid_deriv(self, x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "  def fit(self, X, y, epochs=1000, displayUpdate=100):\n",
        "    X = np.c_[X, np.ones((X.shape[0]))]\n",
        "    for epoch in np.arange(0, epochs):\n",
        "      for (x, target) in zip(X, y):\n",
        "        self.fit_partial(x, target)\n",
        "      if epoch == 0 or (epoch + 1) % displayUpdate == 0:\n",
        "        loss = self.calculate_loss(X, y)\n",
        "        print(\"[INFO] epoch={}, loss={:.7f}\".format(\n",
        "          epoch + 1, loss))\n",
        "      \t\n",
        "  def fit_partial(self, x, y):\n",
        "    A = [np.atleast_2d(x)]\n",
        "    for layer in np.arange(0, len(self.W)):\n",
        "      net = A[layer].dot(self.W[layer])\n",
        "      out = self.sigmoid(net)\n",
        "      A.append(out)\n",
        "    error = A[-1] - y\n",
        "    D = [error * self.sigmoid_deriv(A[-1])]\n",
        "    for layer in np.arange(len(A) - 2, 0, -1):\n",
        "      delta = D[-1].dot(self.W[layer].T)\n",
        "      delta = delta * self.sigmoid_deriv(A[layer])\n",
        "      D.append(delta)\n",
        "    D = D[::-1]\n",
        "    for layer in np.arange(0, len(self.W)):\n",
        "      self.W[layer] += -self.alpha * A[layer].T.dot(D[layer])\n",
        "   \n",
        "  def predict(self, X, addBias=True):\n",
        "    p = np.atleast_2d(X)\n",
        "    if addBias:\n",
        "      p = np.c_[p, np.ones((p.shape[0]))]\n",
        "    for layer in np.arange(0, len(self.W)):\n",
        "      p = self.sigmoid(np.dot(p, self.W[layer]))\n",
        "    return p\n",
        "\n",
        "  def calculate_loss(self, X, targets):\n",
        "    targets = np.array(targets)\n",
        "    predictions = self.predict(X, addBias=False)\n",
        "    predictions = predictions.reshape(-1)\n",
        "    loss = 0.5 * np.sum((predictions - targets) ** 2)\n",
        "    return loss\n",
        "  \n",
        "def collectData(f):\n",
        "  label, input = [], []\n",
        "  res = []\n",
        "  for line in f:\n",
        "    x = line.split(',')\n",
        "    y = []\n",
        "    for item in x:\n",
        "      y.append((item.strip()))\n",
        "    res.append(y)\n",
        "  \n",
        "  for i in range(1, len(res)):\n",
        "      line = res[i]\n",
        "      x = []\n",
        "      y = 0\n",
        "      for item in line:\n",
        "        if(item == 'setosa'):\n",
        "          y = 0\n",
        "        elif(item == 'versicolor'):\n",
        "          y = 1\n",
        "        else:\n",
        "          x.append(float(item))\n",
        "      label.append(y)\n",
        "      input.append(x)\n",
        "  \n",
        "  return np.array(input), np.array(label)\n",
        "\n",
        "X_train, y_train = [], []\n",
        "X_test, y_test = [], []\n",
        "with open('iris_train.csv', 'r') as f:\n",
        "  X_train, y_train = collectData(f)\n",
        "with open('iris_test.csv', 'r') as f:\n",
        "  X_test, y_test = collectData(f)\n",
        "\n",
        "\n",
        "X = X_train\n",
        "y = y_train\n",
        "\n",
        "nn = NeuralNetwork([4, 10, 10, 1], alpha=0.001)\n",
        "nn.fit(X, y, epochs=1000)\n",
        "\n",
        "X = X_test\n",
        "y = y_test\n",
        "\n",
        "true_cases = 0\n",
        "all_cases = 0\n",
        "\n",
        "for (x, target) in zip(X, y):\n",
        "  pred = nn.predict(x)[0][0]\n",
        "  step = 1 if pred > 0.5 else 0\n",
        "  all_cases += 1\n",
        "  if(target == step):\n",
        "    true_cases+=1\n",
        "    \n",
        "acc = true_cases / all_cases\n",
        "print(\"[Final] acc = {}\".format(acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLsZoKn-Eb0d",
        "outputId": "096ce95f-5335-40ff-a917-800c086b2b59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] epoch=1, loss=11.0864545\n",
            "[INFO] epoch=100, loss=9.8823195\n",
            "[INFO] epoch=200, loss=9.7020409\n",
            "[INFO] epoch=300, loss=9.5471500\n",
            "[INFO] epoch=400, loss=9.3553272\n",
            "[INFO] epoch=500, loss=9.1141828\n",
            "[INFO] epoch=600, loss=8.7940903\n",
            "[INFO] epoch=700, loss=8.3327822\n",
            "[INFO] epoch=800, loss=7.6668143\n",
            "[INFO] epoch=900, loss=6.8529715\n",
            "[INFO] epoch=1000, loss=5.9792535\n",
            "[Final] acc = 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "FCrmqQZj6jqs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}